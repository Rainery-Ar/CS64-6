```python
import torch, random, copy
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt

# =========================
# Revert to original settings + key fixes
# =========================
CFG = dict(
    seed=42,
    device="cuda" if torch.cuda.is_available() else "cpu",

    # Original data settings
    num_clients=10,
    classes_per_client=2,
    batch_size=64,                      # Key fix 1: Small batch size for better convergence
    max_items_per_client=3000,          # Reduce data to prevent overfitting

    # Non-IID settings unchanged
    double_imbalance=True,

    # Training settings - no pre-training
    base_pretrain_epochs=0,             # No pre-training as requested
    base_lr=1e-3,
    local_epochs=8,                     # Key fix 2: Greatly increase local epochs
    lr_local=1e-3,                      # Key fix 3: Increase learning rate for more effective LoRA learning
    weight_decay=1e-6,                  # Minimal regularization
    global_rounds=50,

    # LoRA settings - make LoRA more effective
    alpha=16,                           # Key fix 4: Revert to larger alpha for more LoRA influence
    client_ranks=[8,8,8,8,8,8,8,8,8,8], # Key fix 5: Use a uniform, larger rank
    target_rank=8,

    # Other settings
    do_finetune=False,
    figsize=(12,5)
)

# =========================
# Original utility functions - unchanged
# =========================
def set_seed(s):
    random.seed(s); np.random.seed(s)
    torch.manual_seed(s); torch.cuda.manual_seed_all(s)

set_seed(CFG["seed"])
DEVICE = torch.device(CFG["device"])

# =========================
# Original model - unchanged
# =========================
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 10)
        self.drop = nn.Dropout(0.1)
    def forward(self, x):
        x = x.view(-1, 784)
        x = F.relu(self.fc1(x)); x = self.drop(x)
        x = F.relu(self.fc2(x)); x = self.drop(x)
        return self.fc3(x)

class LoRALinear(nn.Module):
    def __init__(self, base_linear: nn.Linear, rank: int, alpha: int):
        super().__init__()
        self.W0 = base_linear
        for p in self.W0.parameters():
            p.requires_grad = False
        self.rank = rank
        self.alpha = alpha
        # Key fix 3: More stable initialization
        self.lora_A = nn.Parameter(torch.randn(rank, self.W0.in_features) * (1.0 / np.sqrt(rank)))
        self.lora_B = nn.Parameter(torch.zeros(self.W0.out_features, rank))
        
    def forward(self, x):
        out = self.W0(x)
        if self.rank > 0:  
            delta = (x @ self.lora_A.t()) @ self.lora_B.t()
            # Key fix 7: Standard LoRA scaling
            return out + (self.alpha / self.rank) * delta  
        return out

def wrap_with_lora(model: nn.Module, rank: int, alpha: int):
    m = copy.deepcopy(model)
    m.fc1 = LoRALinear(m.fc1, rank, alpha)
    m.fc2 = LoRALinear(m.fc2, rank, alpha)
    m.fc3 = LoRALinear(m.fc3, rank, alpha)
    return m

def get_lora_state(model: nn.Module):
    state = {}
    for name in ["fc1","fc2","fc3"]:
        L: LoRALinear = getattr(model, name)
        state[name] = {
            "A": L.lora_A.detach().cpu().clone(),
            "B": L.lora_B.detach().cpu().clone(),
            "in": L.W0.in_features, "out": L.W0.out_features, "rank": L.rank, "alpha": L.alpha
        }
    return state

def broadcast_global_to_client(client_model: nn.Module, global_state):
    for name in ["fc1","fc2","fc3"]:
        Lc: LoRALinear = getattr(client_model, name)
        A_g = global_state[name]["A"]; B_g = global_state[name]["B"]
        r_g = A_g.shape[0]; r_c = Lc.rank
        in_ = Lc.W0.in_features; out_ = Lc.W0.out_features
        
        # Keep original logic, just add robustness checks
        A_new = torch.zeros((r_c, in_), device=A_g.device, dtype=A_g.dtype)
        B_new = torch.zeros((out_, r_c), device=B_g.device, dtype=B_g.dtype)
        r_min = min(r_c, r_g)
        if r_min > 0:  # Key fix 5: Prevent no-op
            A_new[:r_min, :] = A_g[:r_min, :]
            B_new[:, :r_min] = B_g[:, :r_min]

        with torch.no_grad():
            Lc.lora_A.copy_(A_new.to(Lc.lora_A.device))
            Lc.lora_B.copy_(B_new.to(Lc.lora_B.device))

def load_agg_into_global(global_model: nn.Module, agg_state, target_rank:int):
    for name in ["fc1","fc2","fc3"]:
        Lg: LoRALinear = getattr(global_model, name)
        assert Lg.rank == target_rank
        A_ag = agg_state[name]["A"]; B_ag = agg_state[name]["B"]
        r_ag = A_ag.shape[0]
        in_ = Lg.W0.in_features; out_ = Lg.W0.out_features
        
        A_new = torch.zeros((target_rank, in_), device=A_ag.device, dtype=A_ag.dtype)
        B_new = torch.zeros((out_, target_rank), device=B_ag.device, dtype=B_ag.dtype)
        
        r_min = min(target_rank, r_ag)
        if r_min > 0:
            A_new[:r_min, :] = A_ag[:r_min, :]
            B_new[:, :r_min] = B_ag[:, :r_min]
        
        with torch.no_grad():
            Lg.lora_A.copy_(A_new.to(Lg.lora_A.device))
            Lg.lora_B.copy_(B_new.to(Lg.lora_B.device))

# =========================
# Original data processing - unchanged
# =========================
def build_mnist_loaders():
    tfm = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
    train_ds = datasets.MNIST("./data", train=True, download=True, transform=tfm)
    test_ds  = datasets.MNIST("./data", train=False, download=True, transform=tfm)
    return train_ds, test_ds

def indices_by_label_fast(ds):
    if hasattr(ds, "targets"): targets = ds.targets
    elif hasattr(ds, "train_labels"): targets = ds.train_labels
    else: targets = [ds[i][1] for i in range(len(ds))]
    if torch.is_tensor(targets): targets = targets.tolist()
    buckets = defaultdict(list)
    for i, y in enumerate(targets): buckets[int(y)].append(i)
    for k in buckets: random.shuffle(buckets[k])
    return buckets

def make_partitions_double_imbalance(train_ds, num_clients, classes_per_client, max_items_per_client):
    lab2idx = indices_by_label_fast(train_ds)
    probs = np.array([num_clients - i for i in range(10)], dtype=np.float64); probs /= probs.sum()

    client_labels = []
    for _ in range(num_clients):
        labs = np.random.choice(np.arange(10), size=classes_per_client, replace=False, p=probs)
        client_labels.append(sorted(list(map(int, labs))))

    total_train = len(train_ds)
    weights = np.linspace(1.4, 0.6, num_clients); weights /= weights.sum()

    client_indices = []
    for c in range(num_clients):
        pool = []
        for lab in client_labels[c]: pool.extend(lab2idx[lab])
        random.shuffle(pool)
        quota = min(int(total_train * weights[c]), max_items_per_client)
        client_indices.append(pool[:quota])

    return client_indices, client_labels

def make_loader_from_indices(ds, idxs, batch_size, shuffle=True):
    return DataLoader(Subset(ds, idxs), batch_size=batch_size, shuffle=shuffle, drop_last=False)

# =========================
# Train/Test - added stability fixes
# =========================
def train_one_epoch(model, loader, optimizer, device):
    model.train()
    ce = nn.CrossEntropyLoss()
    
    for x,y in loader:
        x,y = x.to(device), y.to(device)
        optimizer.zero_grad()
        out = model(x)
        loss = ce(out, y)
        
        # Check if loss is normal
        if torch.isnan(loss) or torch.isinf(loss):
            print("Warning: NaN or Inf loss detected, skipping batch")
            continue
            
        loss.backward()
        
        # Key fix 8: Moderate gradient clipping
        torch.nn.utils.clip_grad_norm_([p for p in model.parameters() if p.requires_grad], 1.0)
        optimizer.step()

@torch.no_grad()
def evaluate(model, loader, device):
    model.eval()
    tot=0; corr=0
    for x,y in loader:
        x,y = x.to(device), y.to(device)
        pred = model(x).argmax(1)
        tot += y.size(0)
        corr += (pred==y).sum().item()
    return 100.0*corr/tot

# =========================
# Original aggregators - only added numerical stability fixes
# =========================
def zero_pad_product_sum(states, weights, target_rank):
    """Method 1: zero-padding - retains original logic"""
    agg = {}
    for layer in ["fc1","fc2","fc3"]:
        out_ = states[0][layer]["out"]; in_ = states[0][layer]["in"]
        A_sum = torch.zeros((target_rank, in_)); B_sum = torch.zeros((out_, target_rank))
        for st, w in zip(states, weights):
            A = st[layer]["A"]; B = st[layer]["B"]; r = st[layer]["rank"]
            A_pad = torch.zeros((target_rank, in_)); A_pad[:r,:] = A[:r,:]  # Key fix 8: Prevent out-of-bounds access
            B_pad = torch.zeros((out_, target_rank)); B_pad[:,:r] = B[:,:r]
            A_sum += w * A_pad; B_sum += w * B_pad
        agg[layer] = {"A": A_sum, "B": B_sum}
    return agg

def rank_based_product_sum(states, weights, target_rank):
    """Method 2: Product-Sum - retains original logic but with stability fixes"""
    agg = {}
    for layer in ["fc1","fc2","fc3"]:
        out_ = states[0][layer]["out"]; in_ = states[0][layer]["in"]
        
        # Step 1: Zero-pad and weighted average A and B
        A_avg = torch.zeros((target_rank, in_)); B_avg = torch.zeros((out_, target_rank))
        for st, w in zip(states, weights):
            A = st[layer]["A"]; B = st[layer]["B"]; r = st[layer]["rank"]
            A_pad = torch.zeros((target_rank, in_)); A_pad[:r,:] = A[:r,:]
            B_pad = torch.zeros((out_, target_rank)); B_pad[:,:r] = B[:,:r]
            A_avg += w * A_pad; B_avg += w * B_pad
        
        # Step 2: Calculate the averaged delta matrix
        avg_delta = B_avg @ A_avg

        # Step 3: Reconstruct delta matrix with SVD - added numerical stability
        try:
            U, S, Vh = torch.linalg.svd(avg_delta, full_matrices=False)
            
            # Key fix 9: Filter small singular values for stability
            valid_mask = S > 1e-7 * S[0] if len(S) > 0 else torch.zeros_like(S, dtype=torch.bool)
            n_valid = min(target_rank, valid_mask.sum().item())
            
            if n_valid > 0:
                U = U[:, :n_valid]; Vh = Vh[:n_valid, :]; S = S[:n_valid]
                sqrtS = torch.sqrt(torch.clamp(S, min=1e-10))  # Key fix 10: Prevent sqrt(0)
                B_new = U * sqrtS.unsqueeze(0)
                A_new = sqrtS.unsqueeze(1) * Vh
                
                # Pad to target dimension
                A_result = torch.zeros((target_rank, in_))
                B_result = torch.zeros((out_, target_rank))
                A_result[:n_valid, :] = A_new
                B_result[:, :n_valid] = B_new
            else:
                A_result = torch.zeros((target_rank, in_))
                B_result = torch.zeros((out_, target_rank))
                
        except Exception as e:
            print(f"SVD failed for layer {layer}: {e}")
            A_result = torch.zeros((target_rank, in_))
            B_result = torch.zeros((out_, target_rank))
            
        agg[layer] = {"A": A_result, "B": B_result}
    return agg

def svd_sum_product(states, weights, target_rank):
    """Method 3: Sum-Product - retains original logic but with stability fixes"""
    agg = {}
    for layer in ["fc1","fc2","fc3"]:
        in_ = states[0][layer]["in"]; out_ = states[0][layer]["out"]
        DW_sum = torch.zeros((out_, in_))
        for st, w in zip(states, weights):
            DW_sum += w * (st[layer]["B"] @ st[layer]["A"])
        
        try:
            U, S, Vh = torch.linalg.svd(DW_sum, full_matrices=False)
            
            # Key fix 11: Same stability handling as rank_based
            valid_mask = S > 1e-7 * S[0] if len(S) > 0 else torch.zeros_like(S, dtype=torch.bool)
            n_valid = min(target_rank, valid_mask.sum().item())
            
            if n_valid > 0:
                U = U[:, :n_valid]; Vh = Vh[:n_valid, :]; S = S[:n_valid]
                sqrtS = torch.sqrt(torch.clamp(S, min=1e-10))
                B_new = U * sqrtS.unsqueeze(0)
                A_new = sqrtS.unsqueeze(1) * Vh
                
                A_result = torch.zeros((target_rank, in_))
                B_result = torch.zeros((out_, target_rank))
                A_result[:n_valid, :] = A_new
                B_result[:, :n_valid] = B_new
            else:
                A_result = torch.zeros((target_rank, in_))
                B_result = torch.zeros((out_, target_rank))
                
        except Exception as e:
            print(f"SVD failed for layer {layer}: {e}")
            A_result = torch.zeros((target_rank, in_))
            B_result = torch.zeros((out_, target_rank))
            
        agg[layer] = {"A": A_result, "B": B_result}
    return agg

# =========================
# Original federated learning function - minimal changes
# =========================
def run_federated_once(aggregator_name: str, shared_objects=None, seed_offset=0):
    set_seed(CFG["seed"] + seed_offset)

    if shared_objects is None or "train_ds" not in shared_objects:
        train_ds, test_ds = build_mnist_loaders()
        client_indices, client_labels = make_partitions_double_imbalance(
            train_ds, CFG["num_clients"], CFG["classes_per_client"], CFG["max_items_per_client"]
        )
        client_loaders = [make_loader_from_indices(train_ds, idxs, CFG["batch_size"], shuffle=True)
                          for idxs in client_indices]
        client_nsamples = [len(idxs) for idxs in client_indices]
        shared_objects = dict(train_ds=train_ds, test_ds=test_ds,
                              client_indices=client_indices, client_labels=client_labels,
                              client_nsamples=client_nsamples)
    else:
        train_ds = shared_objects["train_ds"]; test_ds = shared_objects["test_ds"]
        client_indices = shared_objects["client_indices"]; client_labels = shared_objects["client_labels"]
        client_nsamples = shared_objects["client_nsamples"]
        client_loaders = [make_loader_from_indices(train_ds, idxs, CFG["batch_size"], shuffle=True)
                          for idxs in client_indices]

    test_loader = DataLoader(test_ds, batch_size=1000, shuffle=False)

    base = MLP().to(DEVICE)
    global_model = wrap_with_lora(base, rank=CFG["target_rank"], alpha=CFG["alpha"]).to(DEVICE)
    init_acc = evaluate(global_model, test_loader, DEVICE)

    client_models = []
    for cid in range(CFG["num_clients"]):
        m = wrap_with_lora(base, rank=CFG["client_ranks"][cid], alpha=CFG["alpha"]).to(DEVICE)
        client_models.append(m)

    curve = []
    best_no_ft = init_acc
    print(f"  Initial accuracy: {init_acc:.2f}%")
    
    for rd in range(1, CFG["global_rounds"]+1):
        g_state = get_lora_state(global_model)
        for m in client_models:
            broadcast_global_to_client(m, g_state)

        returned_states = []; returned_ns = []
        for cid, m in enumerate(client_models):
            # Key fix 9: Use SGD instead of Adam for more stable LoRA training
            opt = optim.SGD([p for p in m.parameters() if p.requires_grad],
                            lr=CFG["lr_local"], weight_decay=CFG["weight_decay"], momentum=0.9)
            
            for _ in range(CFG["local_epochs"]):
                train_one_epoch(m, client_loaders[cid], opt, DEVICE)
                
            returned_states.append(get_lora_state(m))
            returned_ns.append(client_nsamples[cid])

        w = np.array(returned_ns, dtype=np.float64); w = (w / w.sum()).tolist()

        if aggregator_name == "zero_pad_product_sum":
            agg = zero_pad_product_sum(returned_states, w, target_rank=CFG["target_rank"])
        elif aggregator_name == "rank_based_product_sum":
            agg = rank_based_product_sum(returned_states, w, target_rank=CFG["target_rank"])
        elif aggregator_name == "svd_sum_product":
            agg = svd_sum_product(returned_states, w, target_rank=CFG["target_rank"])
        else:
            raise ValueError("Unknown aggregator")

        load_agg_into_global(global_model, agg, target_rank=CFG["target_rank"])

        acc = evaluate(global_model, test_loader, DEVICE)
        best_no_ft = max(best_no_ft, acc)
        curve.append(acc)
        
        # Simplified output: print every 5 rounds
        if rd % 5 == 0 or rd <= 5:
            print(f"  Round {rd:02d}: {acc:.2f}% (Best: {best_no_ft:.2f}%)")

    final_no_ft = curve[-1] if len(curve)>0 else init_acc
    return {
        "init_acc": init_acc,
        "curve": curve,
        "final_no_ft": final_no_ft,
        "best_no_ft": best_no_ft,
        "shared": shared_objects
    }

# =========================
# Main program - retains original structure
# =========================
if __name__ == "__main__":
    set_seed(CFG["seed"])

    train_ds, test_ds = build_mnist_loaders()
    client_indices, client_labels = make_partitions_double_imbalance(
        train_ds, CFG["num_clients"], CFG["classes_per_client"], CFG["max_items_per_client"]
    )
    client_nsamples = [len(idxs) for idxs in client_indices]
    shared = dict(train_ds=train_ds, test_ds=test_ds,
                  client_indices=client_indices, client_labels=client_labels,
                  client_nsamples=client_nsamples)

    print("Client label sets:", client_labels)
    print("Client sample sizes:", client_nsamples)

    results = {}
    for i, agg in enumerate(["zero_pad_product_sum", "rank_based_product_sum", "svd_sum_product"]):
        print(f"\n{'='*50}")
        print(f"Running: {agg}")
        print(f"{'='*50}")
        out = run_federated_once(agg, shared_objects=shared, seed_offset=i+1)
        results[agg] = out

    # Summary of results
    init_acc = results["zero_pad_product_sum"]["init_acc"]
    print(f"\n{'='*60}")
    print("FINAL RESULTS SUMMARY")
    print(f"{'='*60}")
    print(f"Initial global accuracy: {init_acc:.2f}%")
    for agg, out in results.items():
        print(f"[{agg:25s}] Final: {out['final_no_ft']:5.2f}% | Best: {out['best_no_ft']:5.2f}%")

    # Visualization
    plt.figure(figsize=CFG["figsize"])
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c'] 
    labels = ['Zero-Padding', 'Rank-Based', 'SVD Sum-Product']
    
    for i, (agg, out) in enumerate(results.items()):
        plt.plot(range(1, len(out["curve"])+1), out["curve"], 
                 color=colors[i], label=labels[i], linewidth=2, marker='o', markersize=3)
    
    plt.axhline(init_acc, linestyle="--", color='red', alpha=0.7, label="Initial")
    plt.xlabel("Global Round")
    plt.ylabel("Test Accuracy (%)")
    plt.title("Federated LoRA: Three Aggregation Methods")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print("\nExperiment completed!")
```
