{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6qb0zwVu6y1ISjnJlEehn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rainery-Ar/CS64-6/blob/main/3%20tasks%2Cweek9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SV6Aq8z9IOOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cdfcfa7-4661-498c-fe6d-bed59b186234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            " Federated Learning LoRA Heterogeneous Rank Aggregation\n",
            " Theoretical Verification Experiment\n",
            " (Fully Corresponds to Code 1 Logic)\n",
            "================================================================================\n",
            "\n",
            "[Experiment Configuration]\n",
            "Base matrix dimensions: d=10, k=10\n",
            "Global model rank: 100\n",
            "Client ranks: [20, 25, 30]\n",
            "Total rank: 75 < 100\n",
            "Client weights: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
            "\n",
            "[Simulating Client LoRA Parameters]\n",
            "Client 1: A shape=torch.Size([20, 10]), B shape=torch.Size([10, 20]), rank=20\n",
            "Client 2: A shape=torch.Size([25, 10]), B shape=torch.Size([10, 25]), rank=25\n",
            "Client 3: A shape=torch.Size([30, 10]), B shape=torch.Size([10, 30]), rank=30\n",
            "\n",
            "================================================================================\n",
            "[Task 1] SVD Broadcast-based Parameter Stacking Aggregation\n",
            "================================================================================\n",
            "Client ranks: [20, 25, 30]\n",
            "Global stacked rank r_G: 75 (sum of all client ranks)\n",
            "A_stacked shape: torch.Size([75, 10]) (all A vertically stacked)\n",
            "B_stacked shape: torch.Size([10, 75]) (all B horizontally stacked)\n",
            "Delta_W_stacked shape: torch.Size([10, 10])\n",
            "SVD factors: U torch.Size([10, 10]), S torch.Size([10]), Vt torch.Size([10, 10])\n",
            "\n",
            " Task 1 Complete: All client information preserved, global rank=75\n",
            "\n",
            "[Task 1 • Reconstruction Error Table]\n",
            "Target matrix: Delta_W_stacked  |  shape=(10, 10)\n",
            "     r       ||ΔW - UΣVᵀ||_F        RelErr\n",
            "--------------------------------------------\n",
            "     1          6.144662e+01  7.464811e-01\n",
            "    10          5.255772e-05  6.384946e-07\n",
            "    20          5.255772e-05  6.384946e-07\n",
            "    30          5.255772e-05  6.384946e-07\n",
            "\n",
            "================================================================================\n",
            "[Task 2] SVD-based Dimensionality Reduction Reconstruction\n",
            "================================================================================\n",
            "Target rank r_target: 100\n",
            "Actual reconstructed rank r_actual: 10 (limited by min(d,k)=10)\n",
            "Delta_W_agg shape: torch.Size([10, 10])\n",
            "B' shape: torch.Size([10, 10])\n",
            "A' shape: torch.Size([10, 10])\n",
            "Reconstruction error (Frobenius Norm): 0.000017\n",
            "\n",
            " Task 2 Complete: Optimal 10-rank approximation of all client updates\n",
            "\n",
            "[Task 2 • Reconstruction Error Table]\n",
            "Target matrix: Delta_W_agg  |  shape=(10, 10)\n",
            "     r       ||ΔW - UΣVᵀ||_F        RelErr\n",
            "--------------------------------------------\n",
            "     1          2.048221e+01  7.464810e-01\n",
            "    10          1.654527e-05  6.029980e-07\n",
            "    20          1.654527e-05  6.029980e-07\n",
            "    30          1.654527e-05  6.029980e-07\n",
            "\n",
            "================================================================================\n",
            "[Task 3] Engineering Decision Inference: Global High-Rank vs Local Low-Rank\n",
            "================================================================================\n",
            "Global model rank: 100\n",
            "Client ranks: [20, 25, 30]\n",
            "Maximum local rank: 30\n",
            "Rank mismatch: Global 100 > Max local 30\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Inference 1: Zero-Padding/Dilution\n",
            "--------------------------------------------------------------------------------\n",
            "Operation flow:\n",
            "  1. Each client pads A (r_i×10) with zeros to (100×10)\n",
            "  2. FedAvg: A_new = 0.5×A_global + 0.5×Σ(w_i×A_padded_i)\n",
            "\n",
            "Numerical verification (global initial value=5.0):\n",
            "  Updated region (r=1~30): 2.63\n",
            "  Diluted region (r=31~100): 2.50\n",
            "  Theoretical value: 5.0 × 0.5 = 2.50\n",
            "\n",
            "  Problem: Un-updated dimensions from 5.0 → 2.50 (diluted by 50%)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Inference 2: ΔW Aggregation with Truncation\n",
            "--------------------------------------------------------------------------------\n",
            "Operation flow:\n",
            "  1. Compute Delta_W_i = B_i @ A_i (10×10 matrix)\n",
            "  2. FedAvg: ΔW = 0.5×ΔW_global + 0.5×Σ(w_i×ΔW_i)\n",
            "  3. SVD decomposition and reconstruction\n",
            "\n",
            "Numerical verification:\n",
            "  Delta_W shape: torch.Size([10, 10])\n",
            "  SVD actual rank: 10 (= min(10, 10))\n",
            "  A_global_2 shape: torch.Size([100, 10])\n",
            "  B_global_2 shape: torch.Size([10, 100])\n",
            "  Non-zero rank range: r=1~10\n",
            "  Zero rank range: r=11~100\n",
            "\n",
            "  Problem: Global model 90 rank dimensions completely wasted (90%)\n",
            "\n",
            "================================================================================\n",
            "[Experiment Summary]\n",
            "================================================================================\n",
            "\n",
            " Task 1 Verification:\n",
            "  - Stacking aggregation handles heterogeneous ranks, global rank=75\n",
            "  - All client information preserved, no loss\n",
            "\n",
            " Task 2 Verification:\n",
            "  - SVD reconstruction error: 0.000017\n",
            "  - Optimal 10-rank approximation (limited by min(d,k)=10)\n",
            "\n",
            " Task 3 Verification:\n",
            "  - Inference 1 (Zero-padding): Un-updated dimensions diluted by 50%\n",
            "  - Inference 2 (Truncation): 90 rank dimensions wasted (90%)\n",
            "\n",
            " Key Findings:\n",
            "  Scheme 1: Preserves high-rank structure → but dilutes 50% parameter information\n",
            "  Scheme 2: SVD optimal approximation → but wastes 90% rank expressiveness\n",
            "\n",
            "  Fundamental trade-off between:\n",
            "  - High-rank structure vs Information density\n",
            "  - Global consistency vs Local optimality\n",
            "\n",
            "================================================================================\n",
            "Experiment Complete!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\" Federated Learning LoRA Heterogeneous Rank Aggregation\")\n",
        "print(\" Theoretical Verification Experiment\")\n",
        "print(\" (Fully Corresponds to Code 1 Logic)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 0. Experiment Configuration\n",
        "print(\"\\n[Experiment Configuration]\")\n",
        "d, k = 10, 10  # Base matrix dimensions (d_out × d_in for LoRA)\n",
        "R_GLOBAL = 100  # Global model rank\n",
        "client_ranks = [20, 25, 30]  # Client heterogeneous ranks\n",
        "weights = [1/3, 1/3, 1/3]  # Client weights\n",
        "\n",
        "print(f\"Base matrix dimensions: d={d}, k={k}\")\n",
        "print(f\"Global model rank: {R_GLOBAL}\")\n",
        "print(f\"Client ranks: {client_ranks}\")\n",
        "print(f\"Total rank: {sum(client_ranks)} {'<' if sum(client_ranks) < R_GLOBAL else '>'} {R_GLOBAL}\")\n",
        "print(f\"Client weights: {weights}\")\n",
        "\n",
        "# Simulate Client States\n",
        "print(\"\\n[Simulating Client LoRA Parameters]\")\n",
        "client_states = []\n",
        "for i, rank in enumerate(client_ranks):\n",
        "    # A: rank × k (e.g., 20 × 10)\n",
        "    A = torch.randn(rank, k)\n",
        "    # B: d × rank (e.g., 10 × 20)\n",
        "    B = torch.randn(d, rank)\n",
        "    client_states.append({'A': A, 'B': B, 'rank': rank})\n",
        "    print(f\"Client {i+1}: A shape={A.shape}, B shape={B.shape}, rank={rank}\")\n",
        "\n",
        "# Task 1: Parameter Stacking Aggregation\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"[Task 1] SVD Broadcast-based Parameter Stacking Aggregation\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Stack A matrices vertically (row-wise)\n",
        "A_stacked = torch.cat([state['A'] for state in client_states], dim=0)\n",
        "\n",
        "# Stack B matrices horizontally (column-wise)\n",
        "B_stacked = torch.cat([state['B'] for state in client_states], dim=1)\n",
        "\n",
        "# Global stacked rank\n",
        "r_global_stacked = A_stacked.shape[0]\n",
        "\n",
        "# Compute global update matrix\n",
        "Delta_W_stacked = B_stacked @ A_stacked\n",
        "\n",
        "# SVD decomposition for broadcasting\n",
        "U, S, Vt = torch.linalg.svd(Delta_W_stacked)\n",
        "\n",
        "print(f\"Client ranks: {client_ranks}\")\n",
        "print(f\"Global stacked rank r_G: {r_global_stacked} (sum of all client ranks)\")\n",
        "print(f\"A_stacked shape: {A_stacked.shape} (all A vertically stacked)\")\n",
        "print(f\"B_stacked shape: {B_stacked.shape} (all B horizontally stacked)\")\n",
        "print(f\"Delta_W_stacked shape: {Delta_W_stacked.shape}\")\n",
        "print(f\"SVD factors: U {U.shape}, S {S.shape}, Vt {Vt.shape}\")\n",
        "print(f\"\\n Task 1 Complete: All client information preserved, global rank={r_global_stacked}\")\n",
        "\n",
        "def _recon_errors_from_svd(delta_w, U, S, Vt, r_list, title):\n",
        "    print(\"\\n[Task 1 • Reconstruction Error Table]\")\n",
        "    print(f\"Target matrix: {title}  |  shape={tuple(delta_w.shape)}\")\n",
        "    norm_ref = torch.linalg.norm(delta_w).item()\n",
        "    print(f\"{'r':>6}  {'||ΔW - UΣVᵀ||_F':>20}  {'RelErr':>12}\")\n",
        "    print(\"-\" * 44)\n",
        "    for r in r_list:\n",
        "        r_eff = min(r, S.numel())\n",
        "        S_half = torch.diag(torch.sqrt(S[:r_eff]))\n",
        "        B_r = U[:, :r_eff] @ S_half\n",
        "        A_r = S_half @ Vt[:r_eff, :]\n",
        "        rec = B_r @ A_r\n",
        "        err = torch.linalg.norm(delta_w - rec).item()\n",
        "        rel = err / max(norm_ref, 1e-12)\n",
        "        print(f\"{r:6d}  {err:20.6e}  {rel:12.6e}\")\n",
        "\n",
        "# choose representative ranks (1, min/max client rank, and rank ceiling)\n",
        "r_candidates_task1 = sorted(set([1, min(client_ranks), max(client_ranks), min(d, k)]))\n",
        "\n",
        "# Use the SVD you already computed on Delta_W_stacked\n",
        "_recon_errors_from_svd(\n",
        "    delta_w=Delta_W_stacked,\n",
        "    U=U, S=S, Vt=Vt,\n",
        "    r_list=r_candidates_task1,\n",
        "    title=\"Delta_W_stacked\"\n",
        ")\n",
        "\n",
        "\n",
        "# Task 2: SVD Dimensionality Reduction Reconstruction\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"[Task 2] SVD-based Dimensionality Reduction Reconstruction\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Compute complete update matrix for each client\n",
        "delta_W_list = [state['B'] @ state['A'] for state in client_states]\n",
        "\n",
        "# 2. Aggregate all Delta_W (FedAvg)\n",
        "Delta_W_agg = sum(w * dw for w, dw in zip(weights, delta_W_list))\n",
        "\n",
        "# 3. SVD decomposition\n",
        "U, S, Vt = torch.linalg.svd(Delta_W_agg)\n",
        "\n",
        "# 4. Truncate to target rank\n",
        "r_target = R_GLOBAL\n",
        "r_actual = min(r_target, len(S), d, k)  # Actual rank limited by min(d, k)\n",
        "\n",
        "U_r = U[:, :r_actual]\n",
        "S_r = S[:r_actual]\n",
        "Vt_r = Vt[:r_actual, :]\n",
        "\n",
        "# 5. Reconstruct B' and A'\n",
        "S_diag_sqrt = torch.diag(torch.sqrt(S_r))\n",
        "B_prime = U_r @ S_diag_sqrt  # d × r_actual\n",
        "A_prime = S_diag_sqrt @ Vt_r  # r_actual × k\n",
        "\n",
        "# Verify reconstruction error\n",
        "Delta_W_reconstructed = B_prime @ A_prime\n",
        "reconstruction_error = torch.linalg.norm(Delta_W_agg - Delta_W_reconstructed)\n",
        "\n",
        "print(f\"Target rank r_target: {r_target}\")\n",
        "print(f\"Actual reconstructed rank r_actual: {r_actual} (limited by min(d,k)={min(d,k)})\")\n",
        "print(f\"Delta_W_agg shape: {Delta_W_agg.shape}\")\n",
        "print(f\"B' shape: {B_prime.shape}\")\n",
        "print(f\"A' shape: {A_prime.shape}\")\n",
        "print(f\"Reconstruction error (Frobenius Norm): {reconstruction_error.item():.6f}\")\n",
        "#print(f\"Top 5 singular values: {S[:5].numpy()}\")\n",
        "print(f\"\\n Task 2 Complete: Optimal {r_actual}-rank approximation of all client updates\")\n",
        "\n",
        "def _recon_errors_from_svd_T2(delta_w, U, S, Vt, r_list, title):\n",
        "    print(\"\\n[Task 2 • Reconstruction Error Table]\")\n",
        "    print(f\"Target matrix: {title}  |  shape={tuple(delta_w.shape)}\")\n",
        "    norm_ref = torch.linalg.norm(delta_w).item()\n",
        "    print(f\"{'r':>6}  {'||ΔW - UΣVᵀ||_F':>20}  {'RelErr':>12}\")\n",
        "    print(\"-\" * 44)\n",
        "    for r in r_list:\n",
        "        r_eff = min(r, S.numel())\n",
        "        S_half = torch.diag(torch.sqrt(S[:r_eff]))\n",
        "        B_r = U[:, :r_eff] @ S_half\n",
        "        A_r = S_half @ Vt[:r_eff, :]\n",
        "        rec = B_r @ A_r\n",
        "        err = torch.linalg.norm(delta_w - rec).item()\n",
        "        rel = err / max(norm_ref, 1e-12)\n",
        "        print(f\"{r:6d}  {err:20.6e}  {rel:12.6e}\")\n",
        "\n",
        "# representative ranks (same idea; include the actual r you used)\n",
        "r_candidates_task2 = sorted(set([1, min(client_ranks), max(client_ranks), min(d, k)]))\n",
        "\n",
        "# Use the SVD you already computed on Delta_W_agg\n",
        "_recon_errors_from_svd_T2(\n",
        "    delta_w=Delta_W_agg,\n",
        "    U=U, S=S, Vt=Vt,\n",
        "    r_list=r_candidates_task2,\n",
        "    title=\"Delta_W_agg\"\n",
        ")\n",
        "\n",
        "\n",
        "# Task 3: Engineering Decision Inference for Rank Mismatch\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"[Task 3] Engineering Decision Inference: Global High-Rank vs Local Low-Rank\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "max_local_rank = max(client_ranks)\n",
        "print(f\"Global model rank: {R_GLOBAL}\")\n",
        "print(f\"Client ranks: {client_ranks}\")\n",
        "print(f\"Maximum local rank: {max_local_rank}\")\n",
        "print(f\"Rank mismatch: Global {R_GLOBAL} > Max local {max_local_rank}\")\n",
        "\n",
        "#Inference 1: Zero-Padding/Dilution\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Inference 1: Zero-Padding/Dilution\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Simulate global model's initial parameters\n",
        "A_global_init = torch.ones(R_GLOBAL, k) * 5.0\n",
        "B_global_init = torch.ones(d, R_GLOBAL) * 5.0\n",
        "\n",
        "# Each client pads zeros to global rank\n",
        "A_clients_padded = []\n",
        "B_clients_padded = []\n",
        "\n",
        "for state in client_states:\n",
        "    r_local = state['rank']\n",
        "    # Zero padding\n",
        "    A_padded = torch.zeros(R_GLOBAL, k)\n",
        "    A_padded[:r_local, :] = state['A']\n",
        "\n",
        "    B_padded = torch.zeros(d, R_GLOBAL)\n",
        "    B_padded[:, :r_local] = state['B']\n",
        "\n",
        "    A_clients_padded.append(A_padded)\n",
        "    B_clients_padded.append(B_padded)\n",
        "\n",
        "# FedAvg mixing\n",
        "weight_global = 0.5\n",
        "weight_local = 0.5\n",
        "\n",
        "A_scheme1 = weight_global * A_global_init\n",
        "B_scheme1 = weight_global * B_global_init\n",
        "\n",
        "for A_pad, B_pad, w in zip(A_clients_padded, B_clients_padded, weights):\n",
        "    A_scheme1 += weight_local * w * A_pad\n",
        "    B_scheme1 += weight_local * w * B_pad\n",
        "\n",
        "# Verify dilution effect\n",
        "val_updated = A_scheme1[0, 0].item()  # Updated region\n",
        "val_diluted = A_scheme1[max_local_rank, 0].item()  # Diluted region\n",
        "\n",
        "print(f\"Operation flow:\")\n",
        "print(f\"  1. Each client pads A (r_i×{k}) with zeros to ({R_GLOBAL}×{k})\")\n",
        "print(f\"  2. FedAvg: A_new = {weight_global}×A_global + {weight_local}×Σ(w_i×A_padded_i)\")\n",
        "print(f\"\\nNumerical verification (global initial value=5.0):\")\n",
        "print(f\"  Updated region (r=1~{max_local_rank}): {val_updated:.2f}\")\n",
        "print(f\"  Diluted region (r={max_local_rank+1}~{R_GLOBAL}): {val_diluted:.2f}\")\n",
        "print(f\"  Theoretical value: 5.0 × {weight_global} = {5.0 * weight_global:.2f}\")\n",
        "print(f\"\\n  Problem: Un-updated dimensions from 5.0 → {val_diluted:.2f} (diluted by {(5.0-val_diluted)/5.0*100:.0f}%)\")\n",
        "\n",
        "#Inference 2: Truncation/Discarding High-Rank\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"Inference 2: ΔW Aggregation with Truncation\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Compute and aggregate Delta_W\n",
        "delta_W_list_2 = [state['B'] @ state['A'] for state in client_states]\n",
        "Delta_W_global = sum(w * dw for w, dw in zip(weights, delta_W_list_2))\n",
        "\n",
        "# Mix with global Delta_W\n",
        "Delta_W_global_init = torch.ones(d, k) * 0.1\n",
        "Delta_W_scheme2 = weight_global * Delta_W_global_init + weight_local * Delta_W_global\n",
        "\n",
        "# SVD decomposition\n",
        "U2, S2, Vt2 = torch.linalg.svd(Delta_W_scheme2)\n",
        "\n",
        "# Actual rank limited by min(d, k)\n",
        "r_slice = min(d, k)\n",
        "\n",
        "U_final = U2[:, :r_slice]\n",
        "S_final = S2[:r_slice]\n",
        "Vt_final = Vt2[:r_slice, :]\n",
        "\n",
        "# Reconstruct A and B\n",
        "S_diag_sqrt2 = torch.diag(torch.sqrt(S_final))\n",
        "B_reconstructed = U_final @ S_diag_sqrt2\n",
        "A_reconstructed = S_diag_sqrt2 @ Vt_final\n",
        "\n",
        "# Pad to global rank\n",
        "A_global_2 = torch.zeros(R_GLOBAL, k)\n",
        "A_global_2[:r_slice, :] = A_reconstructed\n",
        "\n",
        "B_global_2 = torch.zeros(d, R_GLOBAL)\n",
        "B_global_2[:, :r_slice] = B_reconstructed\n",
        "\n",
        "wasted_rank = R_GLOBAL - r_slice\n",
        "\n",
        "print(f\"Operation flow:\")\n",
        "print(f\"  1. Compute Delta_W_i = B_i @ A_i ({d}×{k} matrix)\")\n",
        "print(f\"  2. FedAvg: ΔW = {weight_global}×ΔW_global + {weight_local}×Σ(w_i×ΔW_i)\")\n",
        "print(f\"  3. SVD decomposition and reconstruction\")\n",
        "print(f\"\\nNumerical verification:\")\n",
        "print(f\"  Delta_W shape: {Delta_W_scheme2.shape}\")\n",
        "print(f\"  SVD actual rank: {r_slice} (= min({d}, {k}))\")\n",
        "print(f\"  A_global_2 shape: {A_global_2.shape}\")\n",
        "print(f\"  B_global_2 shape: {B_global_2.shape}\")\n",
        "print(f\"  Non-zero rank range: r=1~{r_slice}\")\n",
        "print(f\"  Zero rank range: r={r_slice+1}~{R_GLOBAL}\")\n",
        "print(f\"\\n  Problem: Global model {wasted_rank} rank dimensions completely wasted ({wasted_rank/R_GLOBAL*100:.0f}%)\")\n",
        "\n",
        "# Final Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"[Experiment Summary]\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n Task 1 Verification:\")\n",
        "print(f\"  - Stacking aggregation handles heterogeneous ranks, global rank={r_global_stacked}\")\n",
        "print(f\"  - All client information preserved, no loss\")\n",
        "\n",
        "print(\"\\n Task 2 Verification:\")\n",
        "print(f\"  - SVD reconstruction error: {reconstruction_error.item():.6f}\")\n",
        "print(f\"  - Optimal {r_actual}-rank approximation (limited by min(d,k)={min(d,k)})\")\n",
        "\n",
        "print(\"\\n Task 3 Verification:\")\n",
        "print(f\"  - Inference 1 (Zero-padding): Un-updated dimensions diluted by {(5.0-val_diluted)/5.0*100:.0f}%\")\n",
        "print(f\"  - Inference 2 (Truncation): {wasted_rank} rank dimensions wasted ({wasted_rank/R_GLOBAL*100:.0f}%)\")\n",
        "\n",
        "print(\"\\n Key Findings:\")\n",
        "print(f\"  Scheme 1: Preserves high-rank structure → but dilutes {(5.0-val_diluted)/5.0*100:.0f}% parameter information\")\n",
        "print(f\"  Scheme 2: SVD optimal approximation → but wastes {wasted_rank/R_GLOBAL*100:.0f}% rank expressiveness\")\n",
        "print(f\"\\n  Fundamental trade-off between:\")\n",
        "print(f\"  - High-rank structure vs Information density\")\n",
        "print(f\"  - Global consistency vs Local optimality\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Experiment Complete!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ]
}